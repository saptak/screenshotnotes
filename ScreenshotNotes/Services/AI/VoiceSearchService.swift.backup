import Foundation
import Speech
import AVFoundation
import Combine

#if canImport(UIKit)
import UIKit
#endif

/// Comprehensive voice search service leveraging Speech Framework
/// Provides real-time voice-to-text with intelligent search optimization
@MainActor
public class VoiceSearchService: NSObject, ObservableObject {
    
    // MARK: - Published Properties
    
    @Published public private(set) var isListening = false
    @Published public private(set) var transcribedText = ""
    @Published public private(set) var isProcessing = false
    @Published public private(set) var authorizationStatus: SFSpeechRecognizerAuthorizationStatus = .notDetermined
    @Published public private(set) var lastError: VoiceSearchError?
    @Published public private(set) var confidenceLevel: Float = 0.0
    @Published public private(set) var speechRate: Float = 0.0
    @Published public private(set) var hasAudioInput = false
    
    // MARK: - Private Properties
    
    private let speechRecognizer: SFSpeechRecognizer
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let locale: Locale
    
    // Audio configuration
    #if os(iOS)
    private let audioSession = AVAudioSession.sharedInstance()
    #endif
    private var inputNode: AVAudioInputNode?
    private let recordingFormat: AVAudioFormat
    
    // Processing and optimization
    private var processingTimer: Timer?
    private var silenceTimer: Timer?
    private var transcriptionBuffer: [String] = []
    private let maxBufferSize = 10
    private let silenceThreshold: TimeInterval = 2.0
    private let minSpeechDuration: TimeInterval = 0.5
    
    // Performance metrics
    private var startTime: Date?
    private var lastTranscriptionTime: Date?
    private var sessionMetrics = VoiceSessionMetrics()
    
    // MARK: - Configuration
    
    private let speechTimeout: TimeInterval = 60.0
    private let recognitionTimeout: TimeInterval = 30.0
    private let audioLevelUpdateInterval: TimeInterval = 0.1
    
    // MARK: - Initialization
    
    public override init() {
        // Configure for user's preferred language with fallback to English
        self.locale = Locale.current
        self.speechRecognizer = SFSpeechRecognizer(locale: locale) ?? SFSpeechRecognizer(locale: Locale(identifier: "en-US"))!
        
        // Configure audio format for optimal speech recognition
        self.recordingFormat = AVAudioFormat(
            standardFormatWithSampleRate: 16000,
            channels: 1
        )!
        
        super.init()
        
        // Set delegate for speech recognizer events
        speechRecognizer.delegate = self
        
        // Initialize authorization status
        authorizationStatus = SFSpeechRecognizer.authorizationStatus()
        
        // Configure audio session for speech recognition
        configureAudioSession()
        
        print("ðŸŽ¤ VoiceSearchService initialized with locale: \(locale.identifier)")
    }
    
    deinit {
        Task { @MainActor [weak self] in
            self?.stopListening()
        }
        audioEngine.stop()
        recognitionTask?.cancel()
    }
    
    // MARK: - Public Methods
    
    /// Request necessary permissions for voice search
    public func requestPermissions() async -> Bool {
        let speechAuthStatus = await requestSpeechRecognitionAuthorization()
        let microphoneAuthStatus = await requestMicrophoneAuthorization()
        
        await MainActor.run {
            hasAudioInput = microphoneAuthStatus
            authorizationStatus = speechAuthStatus
        }
        
        return speechAuthStatus == .authorized && microphoneAuthStatus
    }
    
    /// Start listening for voice input
    public func startListening() async throws {
        guard authorizationStatus == .authorized else {
            throw VoiceSearchError.notAuthorized
        }
        
        guard !audioEngine.isRunning else {
            print("ðŸŽ¤ Audio engine already running")
            return
        }
        
        // Reset state
        await MainActor.run {
            transcribedText = ""
            lastError = nil
            isProcessing = true
            confidenceLevel = 0.0
            speechRate = 0.0
        }
        
        try await setupAudioEngine()
        try await startSpeechRecognition()
        
        await MainActor.run {
            isListening = true
            isProcessing = false
            startTime = Date()
            sessionMetrics = VoiceSessionMetrics()
        }
        
        print("ðŸŽ¤ Voice listening started successfully")
    }
    
    /// Stop listening and finalize transcription
    public func stopListening() {
        guard isListening else { return }
        
        // Stop timers
        processingTimer?.invalidate()
        silenceTimer?.invalidate()
        
        // Stop audio engine
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // Finalize recognition
        recognitionRequest?.endAudio()
        recognitionTask?.finish()
        
        // Update state
        isListening = false
        isProcessing = false
        
        // Calculate session metrics
        if let startTime = startTime {
            sessionMetrics.duration = Date().timeIntervalSince(startTime)
            sessionMetrics.finalTranscription = transcribedText
            sessionMetrics.averageConfidence = confidenceLevel
        }
        
        print("ðŸŽ¤ Voice listening stopped. Final transcription: '\(transcribedText)'")
        print("ðŸ“Š Session metrics: \(sessionMetrics)")
    }
    
    /// Cancel listening without processing
    public func cancelListening() {
        recognitionTask?.cancel()
        stopListening()
        
        transcribedText = ""
        lastError = nil
        
        print("ðŸŽ¤ Voice listening cancelled")
    }
    
    /// Process raw transcription for search optimization
    public func optimizeForSearch(_ rawText: String) -> String {
        var optimized = rawText.trimmingCharacters(in: .whitespacesAndNewlines)
        
        // Normalize common speech patterns
        optimized = normalizeSpeechPatterns(optimized)
        
        // Remove filler words
        optimized = removeFillerWords(optimized)
        
        // Enhance for search context
        optimized = enhanceSearchContext(optimized)
        
        return optimized
    }
    
    /// Get current authorization status
    public func checkAuthorizationStatus() -> VoiceAuthorizationStatus {
        let speechAuth = SFSpeechRecognizer.authorizationStatus()
        
        #if os(iOS)
        if #available(iOS 17.0, *) {
            let microphoneAuth = AVAudioApplication.shared.recordPermission
            // Convert AVAudioApplication.recordPermission to AVAudioSession.RecordPermission
            let convertedAuth: AVAudioSession.RecordPermission
            switch microphoneAuth {
            case .granted:
                convertedAuth = .granted
            case .denied:
                convertedAuth = .denied
            case .undetermined:
                convertedAuth = .undetermined
            @unknown default:
                convertedAuth = .undetermined
            }
            return VoiceAuthorizationStatus(
                speechRecognition: speechAuth,
                microphone: convertedAuth
            )
        } else {
            let microphoneAuth = AVAudioSession.sharedInstance().recordPermission
            return VoiceAuthorizationStatus(
                speechRecognition: speechAuth,
                microphone: microphoneAuth
            )
        }
        #else
        // macOS fallback - assume granted for now
        return VoiceAuthorizationStatus(
            speechRecognition: speechAuth,
            microphone: true
        )
        #endif
    }
    
    // MARK: - Private Methods
    
    private func configureAudioSession() {
        #if os(iOS)
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("âŒ Failed to configure audio session: \(error)")
            lastError = .audioSessionError(error)
        }
        #endif
    }
    
    private func setupAudioEngine() async throws {
        // Stop any existing recognition
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // Configure input node
        inputNode = audioEngine.inputNode
        
        guard let inputNode = inputNode else {
            throw VoiceSearchError.audioEngineError("No audio input available")
        }
        
        // Remove existing tap
        inputNode.removeTap(onBus: 0)
        
        // Install tap with optimized format
        let inputFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
            
            // Monitor audio levels for visual feedback
            Task { @MainActor in
                self?.updateAudioLevels(from: buffer)
            }
        }
        
        // Prepare and start audio engine
        audioEngine.prepare()
        try audioEngine.start()
    }
    
    private func startSpeechRecognition() async throws {
        // Create recognition request
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else {
            throw VoiceSearchError.recognitionRequestFailed
        }
        
        // Configure recognition request
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.requiresOnDeviceRecognition = true // Prefer on-device for privacy
        recognitionRequest.addsPunctuation = true
        
        // Start recognition task
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            Task { @MainActor in
                await self?.handleRecognitionResult(result: result, error: error)
            }
        }
        
        // Set up timeout handling
        setupTimeoutHandling()
    }
    
    private func handleRecognitionResult(result: SFSpeechRecognitionResult?, error: Error?) async {
        
        if let error = error {
            await handleRecognitionError(error)
            return
        }
        
        guard let result = result else { return }
        
        // Update transcription
        let newTranscription = result.bestTranscription.formattedString
        transcribedText = newTranscription
        lastTranscriptionTime = Date()
        
        // Update confidence and speech rate
        confidenceLevel = result.bestTranscription.segments.isEmpty ? 0.0 : 
            result.bestTranscription.segments.map(\.confidence).reduce(0, +) / Float(result.bestTranscription.segments.count)
        
        speechRate = calculateSpeechRate(from: result.bestTranscription)
        
        // Update session metrics
        sessionMetrics.transcriptionUpdates += 1
        sessionMetrics.currentConfidence = confidenceLevel
        
        // Handle final result
        if result.isFinal {
            await processFinalTranscription(newTranscription)
        } else {
            // Reset silence timer for partial results
            resetSilenceTimer()
        }
        
        print("ðŸŽ¤ Transcription update: '\(newTranscription)' (confidence: \(confidenceLevel))")
    }
    
    private func handleRecognitionError(_ error: Error) async {
        print("âŒ Speech recognition error: \(error)")
        
        let voiceError: VoiceSearchError
        let speechError = error as NSError
        switch speechError.code {
        case 203: // Network unavailable
            voiceError = .networkError
        case 216: // Recognition service busy
            voiceError = .serviceUnavailable
        default:
            voiceError = .recognitionFailed(error)
        }
        
        lastError = voiceError
        stopListening()
    }
    
    private func processFinalTranscription(_ text: String) async {
        guard !text.isEmpty else { return }
        
        let optimizedText = optimizeForSearch(text)
        transcribedText = optimizedText
        
        sessionMetrics.finalTranscription = optimizedText
        sessionMetrics.processingComplete = true
        
        print("ðŸŽ¤ Final transcription processed: '\(optimizedText)'")
    }
    
    private func setupTimeoutHandling() {
        // Overall session timeout
        processingTimer = Timer.scheduledTimer(withTimeInterval: speechTimeout, repeats: false) { [weak self] _ in
            Task { @MainActor in
                self?.handleTimeout()
            }
        }
        
        // Start silence detection
        resetSilenceTimer()
    }
    
    private func resetSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceThreshold, repeats: false) { [weak self] _ in
            Task { @MainActor in
                self?.handleSilenceTimeout()
            }
        }
    }
    
    private func handleTimeout() {
        print("ðŸŽ¤ Voice session timeout")
        lastError = .sessionTimeout
        stopListening()
    }
    
    private func handleSilenceTimeout() {
        print("ðŸŽ¤ Silence timeout - stopping listening")
        stopListening()
    }
    
    private func updateAudioLevels(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData else { return }
        
        let frameLength = Int(buffer.frameLength)
        let samples = Array(UnsafeBufferPointer(start: channelData[0], count: frameLength))
        
        // Calculate RMS level
        let rms = sqrt(samples.map { $0 * $0 }.reduce(0, +) / Float(frameLength))
        let level = 20 * log10(rms + 0.000001) // Add small value to avoid log(0)
        
        // Normalize to 0-1 range (assuming -60dB to 0dB)
        let _ = max(0, min(1, (level + 60) / 60))
        
        // Update audio level for UI feedback
        DispatchQueue.main.async {
            // This could be used for visual audio level indicators
        }
    }
    
    private func calculateSpeechRate(from transcription: SFTranscription) -> Float {
        guard !transcription.segments.isEmpty else { return 0.0 }
        
        let totalDuration = transcription.segments.last?.timestamp ?? 0.0
        let wordCount = transcription.segments.count
        
        return totalDuration > 0 ? Float(wordCount) / Float(totalDuration) : 0.0
    }
    
    // MARK: - Text Processing Methods
    
    private func normalizeSpeechPatterns(_ text: String) -> String {
        var normalized = text
        
        // Common speech-to-text corrections
        let patterns = [
            ("show me", "find"),
            ("look for", "find"),
            ("search for", "find"),
            ("get me", "find"),
            ("I want", "find"),
            ("I need", "find"),
            ("can you find", "find"),
            ("please find", "find")
        ]
        
        for (pattern, replacement) in patterns {
            normalized = normalized.replacingOccurrences(
                of: pattern,
                with: replacement,
                options: [.caseInsensitive, .anchored]
            )
        }
        
        return normalized
    }
    
    private func removeFillerWords(_ text: String) -> String {
        let fillerWords = Set([
            "um", "uh", "ah", "like", "you know", "basically", "actually",
            "literally", "so", "well", "okay", "right", "I mean"
        ])
        
        let words = text.components(separatedBy: .whitespacesAndNewlines)
        let filteredWords = words.filter { word in
            !fillerWords.contains(word.lowercased().trimmingCharacters(in: .punctuationCharacters))
        }
        
        return filteredWords.joined(separator: " ")
    }
    
    private func enhanceSearchContext(_ text: String) -> String {
        let enhanced = text
        
        // Add context for screenshot-specific searches
        if !enhanced.lowercased().contains("screenshot") && 
           !enhanced.lowercased().contains("image") &&
           !enhanced.lowercased().contains("photo") {
            
            // Infer screenshot context from search terms
            let screenshotIndicators = ["receipt", "document", "text", "email", "message", "website"]
            let hasIndicator = screenshotIndicators.contains { indicator in
                enhanced.lowercased().contains(indicator)
            }
            
            if hasIndicator {
                // Context is implied, don't add redundant terms
            }
        }
        
        return enhanced.trimmingCharacters(in: .whitespacesAndNewlines)
    }
    
    // MARK: - Permission Methods
    
    private func requestSpeechRecognitionAuthorization() async -> SFSpeechRecognizerAuthorizationStatus {
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
    }
    
    private func requestMicrophoneAuthorization() async -> Bool {
        return await withCheckedContinuation { continuation in
            #if os(iOS)
            if #available(iOS 17.0, *) {
                AVAudioApplication.requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            } else {
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            }
            #else
            // macOS fallback - assume granted for now
            continuation.resume(returning: true)
            #endif
        }
    }
}

// MARK: - SFSpeechRecognizerDelegate

extension VoiceSearchService: SFSpeechRecognizerDelegate {
    nonisolated public func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        DispatchQueue.main.async {
            if !available {
                self.lastError = .serviceUnavailable
                self.stopListening()
            }
        }
        
        print("ðŸŽ¤ Speech recognizer availability changed: \(available)")
    }
}

// MARK: - Supporting Types

public enum VoiceSearchError: LocalizedError, Equatable {
    case notAuthorized
    case audioSessionError(Error)
    case audioEngineError(String)
    case recognitionRequestFailed
    case recognitionFailed(Error)
    case networkError
    case serviceUnavailable
    case sessionTimeout
    case microphoneUnavailable
    
    public var errorDescription: String? {
        switch self {
        case .notAuthorized:
            return "Speech recognition not authorized. Please enable in Settings."
        case .audioSessionError:
            return "Audio session configuration failed. Please check microphone access."
        case .audioEngineError(let message):
            return "Audio engine error: \(message)"
        case .recognitionRequestFailed:
            return "Failed to create speech recognition request."
        case .recognitionFailed:
            return "Speech recognition failed. Please try again."
        case .networkError:
            return "Network unavailable for speech recognition."
        case .serviceUnavailable:
            return "Speech recognition service temporarily unavailable."
        case .sessionTimeout:
            return "Voice session timed out. Please try again."
        case .microphoneUnavailable:
            return "Microphone access required for voice search."
        }
    }
    
    public static func == (lhs: VoiceSearchError, rhs: VoiceSearchError) -> Bool {
        switch (lhs, rhs) {
        case (.notAuthorized, .notAuthorized),
             (.recognitionRequestFailed, .recognitionRequestFailed),
             (.networkError, .networkError),
             (.serviceUnavailable, .serviceUnavailable),
             (.sessionTimeout, .sessionTimeout),
             (.microphoneUnavailable, .microphoneUnavailable):
            return true
        case (.audioEngineError(let lhsMessage), .audioEngineError(let rhsMessage)):
            return lhsMessage == rhsMessage
        default:
            return false
        }
    }
}

public struct VoiceAuthorizationStatus {
    public let speechRecognition: SFSpeechRecognizerAuthorizationStatus
    
    #if os(iOS)
    public let microphone: AVAudioSession.RecordPermission
    
    public var isFullyAuthorized: Bool {
        return speechRecognition == .authorized && microphone == .granted
    }
    
    public var requiresSetup: Bool {
        return speechRecognition == .notDetermined || microphone == .undetermined
    }
    
    public init(speechRecognition: SFSpeechRecognizerAuthorizationStatus, microphone: AVAudioSession.RecordPermission) {
        self.speechRecognition = speechRecognition
        self.microphone = microphone
    }
    #else
    // macOS version - no microphone property
    public var isFullyAuthorized: Bool {
        return speechRecognition == .authorized
    }
    
    public var requiresSetup: Bool {
        return speechRecognition == .notDetermined
    }
    
    public init(speechRecognition: SFSpeechRecognizerAuthorizationStatus, microphone: Bool) {
        self.speechRecognition = speechRecognition
        // microphone parameter ignored on macOS
    }
    #endif
}

public struct VoiceSessionMetrics {
    public var duration: TimeInterval = 0
    public var transcriptionUpdates: Int = 0
    public var currentConfidence: Float = 0
    public var averageConfidence: Float = 0
    public var finalTranscription: String = ""
    public var processingComplete: Bool = false
    
    public var wordsPerMinute: Float {
        let words = finalTranscription.components(separatedBy: .whitespacesAndNewlines).count
        return duration > 0 ? Float(words) / Float(duration / 60) : 0
    }
}